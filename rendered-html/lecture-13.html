<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>lecture-13</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="/Users/eugen/Library/Application Support/abnerworks.Typora/themes/vue.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="lekce-13">Lekce 13</h1>
<p>Show how to incrementally update a running average (how to compute an average of <span class="math inline">\(N\)</span> numbers using the average of the first <span class="math inline">\(N-1\)</span> numbers). [5]</p>
<p>Describe multi-arm bandits and write down the <span class="math inline">\(\varepsilon\)</span>-greedy algorithm for solving it. [5]</p>
<p>Define the Markov Decision Process, including the definition of the return. [5]</p>
<p>Define the value function, such that all expectations are over simple random variables (actions, states, rewards), not trajectories. [5]</p>
<p>Define the action-value function, such that all expectations are over simple random variables (actions, states, rewards), not trajectories. [5]</p>
<p>Express the value function using the action-value function, and express the action-value function using the value function. [5]</p>
<p>Define the optimal value function and the optimal action-value function. Then define optimal policy in such a way that its existence is guaranteed. [5]</p>
<p>Write down the Monte-Carlo on-policy every-visit <span class="math inline">\(\varepsilon\)</span>-soft algorithm. [10]</p>
<p>Formulate the policy gradient theorem. [5]</p>
<p>Prove the part of the policy gradient theorem showing the value of <span class="math inline">\(\nabla_\theta v_\pi (s)\)</span>. [10]</p>
<p>Assuming the policy gradient theorem, formulate the loss used by the REINFORCE algorithm and show how can its gradient be expressed as an expectation over states and actions. [5]</p>
<p>Write down the REINFORCE algorithm. [10]</p>
<p>Show that introducing baseline does not influence validity of the policy gradient theorem. [5]</p>
<p>Write down the REINFORCE with baseline algorithm. [10]</p>
</body>
</html>
